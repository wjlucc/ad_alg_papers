#!/usr/bin/env python3
"""
è®ºæ–‡æ‰¹é‡ä¸‹è½½å·¥å…·
æ”¯æŒä» arXiv å’Œ Semantic Scholar æœç´¢å¹¶ä¸‹è½½è®ºæ–‡PDF

ä½¿ç”¨æ–¹æ³•:
    python paper_downloader.py                    # äº¤äº’å¼å•ç¯‡æœç´¢ä¸‹è½½
    python paper_downloader.py --batch papers.txt # æ‰¹é‡ä¸‹è½½ï¼ˆä»æ–‡ä»¶è¯»å–æ ‡é¢˜åˆ—è¡¨ï¼‰
    python paper_downloader.py --from-readme      # ä»readme.mdè§£æè®ºæ–‡æ ‡é¢˜å¹¶ä¸‹è½½
"""

import os
import re
import html
import json
import time
import urllib.request
import urllib.parse
import urllib.error
from pathlib import Path
from dataclasses import dataclass
from difflib import SequenceMatcher
from typing import Optional
import ssl

# åˆ›å»ºä¸éªŒè¯SSLçš„ä¸Šä¸‹æ–‡ï¼ˆæŸäº›ç¯å¢ƒå¯èƒ½éœ€è¦ï¼‰
SSL_CONTEXT = ssl.create_default_context()
SSL_CONTEXT.check_hostname = False
SSL_CONTEXT.verify_mode = ssl.CERT_NONE

# ç›®å½•æ˜ å°„ï¼šæ¿å—ç¼–å· -> ç›®å½•å
CATEGORY_MAP = {
    "1": "1_ç«ä»·ç­–ç•¥",
    "2": "2_æ‹å–æœºåˆ¶è®¾è®¡", 
    "3": "3_LLMä¸ç»æµä»£ç†",
    "4": "4_åšå¼ˆè®ºåŸºç¡€",
    "5": "5_åŸºå‡†ä¸ç»¼è¿°",
}

# é»˜è®¤è¾“å‡ºç›®å½•
DEFAULT_OUTPUT_DIR = "./Ad_Bidding_Auction_Mechanisms"


@dataclass
class Paper:
    """è®ºæ–‡ä¿¡æ¯ç»“æ„"""
    title: str
    authors: list[str]
    year: Optional[int]
    pdf_url: Optional[str]
    arxiv_id: Optional[str]
    source: str  # 'arxiv' or 'semantic_scholar'
    abstract: Optional[str] = None


class SemanticScholarAPI:
    """Semantic Scholar API å°è£…"""
    BASE_URL = "https://api.semanticscholar.org/graph/v1"
    
    def __init__(self, api_key: Optional[str] = None):
        self.api_key = api_key
        self.headers = {"Accept": "application/json"}
        if api_key:
            self.headers["x-api-key"] = api_key
    
    def search(self, query: str, limit: int = 5) -> list[Paper]:
        """æœç´¢è®ºæ–‡"""
        params = urllib.parse.urlencode({
            "query": query,
            "limit": limit,
            "fields": "title,authors,year,openAccessPdf,externalIds,abstract"
        })
        url = f"{self.BASE_URL}/paper/search?{params}"

        last_error: Optional[Exception] = None
        for attempt in range(1, 5):
            try:
                req = urllib.request.Request(url, headers=self.headers)
                with urllib.request.urlopen(req, timeout=30, context=SSL_CONTEXT) as response:
                    data = json.loads(response.read().decode('utf-8'))
                last_error = None
                break
            except urllib.error.HTTPError as e:
                last_error = e
                if e.code == 429 and attempt < 5:
                    retry_after = e.headers.get("Retry-After") if getattr(e, "headers", None) else None
                    wait_s = int(retry_after) if retry_after and str(retry_after).isdigit() else min(2**attempt, 30)
                    print(f"âš ï¸  Semantic Scholar è§¦å‘é™æµ(429)ï¼Œç­‰å¾… {wait_s}s åé‡è¯•... ({attempt}/5)")
                    time.sleep(wait_s)
                    continue
                print(f"âš ï¸  Semantic Scholar API é”™è¯¯: {e.code}")
                return []
            except Exception as e:
                last_error = e
                break

        if last_error is not None:
            print(f"âš ï¸  è¯·æ±‚å¤±è´¥: {last_error}")
            return []
        
        papers = []
        for item in data.get("data", []):
            arxiv_id = None
            if item.get("externalIds"):
                arxiv_id = item["externalIds"].get("ArXiv")
            
            pdf_url = None
            if item.get("openAccessPdf"):
                pdf_url = item["openAccessPdf"].get("url")
            if not pdf_url and arxiv_id:
                pdf_url = f"https://arxiv.org/pdf/{arxiv_id}.pdf"
            
            papers.append(Paper(
                title=item.get("title", ""),
                authors=[a.get("name", "") for a in item.get("authors", [])],
                year=item.get("year"),
                pdf_url=pdf_url,
                arxiv_id=arxiv_id,
                source="semantic_scholar",
                abstract=item.get("abstract")
            ))
        
        return papers


class ArxivAPI:
    """arXiv API å°è£…"""
    BASE_URL = "http://export.arxiv.org/api/query"
    
    def search(self, query: str, max_results: int = 5) -> list[Paper]:
        """æœç´¢è®ºæ–‡"""
        params = urllib.parse.urlencode({
            "search_query": f"all:{query}",
            "start": 0,
            "max_results": max_results,
            "sortBy": "relevance"
        })
        url = f"{self.BASE_URL}?{params}"
        
        try:
            req = urllib.request.Request(url)
            with urllib.request.urlopen(req, timeout=30) as response:
                content = response.read().decode('utf-8')
        except Exception as e:
            print(f"âš ï¸  arXiv API è¯·æ±‚å¤±è´¥: {e}")
            return []
        
        # ç®€å•çš„XMLè§£æï¼ˆé¿å…ä¾èµ–é¢å¤–åº“ï¼‰
        papers = []
        entries = re.findall(r'<entry>(.*?)</entry>', content, re.DOTALL)
        
        for entry in entries:
            # æå–æ ‡é¢˜
            title_match = re.search(r'<title>(.*?)</title>', entry, re.DOTALL)
            title = title_match.group(1).strip().replace('\n', ' ') if title_match else ""
            
            # æå–ä½œè€…
            authors = re.findall(r'<name>(.*?)</name>', entry)
            
            # æå–IDå’ŒPDFé“¾æ¥
            id_match = re.search(r'<id>http://arxiv.org/abs/(.*?)</id>', entry)
            arxiv_id = id_match.group(1) if id_match else None
            pdf_url = f"https://arxiv.org/pdf/{arxiv_id}.pdf" if arxiv_id else None
            
            # æå–å¹´ä»½
            published_match = re.search(r'<published>(\d{4})', entry)
            year = int(published_match.group(1)) if published_match else None
            
            # æå–æ‘˜è¦
            abstract_match = re.search(r'<summary>(.*?)</summary>', entry, re.DOTALL)
            abstract = abstract_match.group(1).strip() if abstract_match else None
            
            if title:
                papers.append(Paper(
                    title=title,
                    authors=authors,
                    year=year,
                    pdf_url=pdf_url,
                    arxiv_id=arxiv_id,
                    source="arxiv",
                    abstract=abstract
                ))
        
        return papers


class PaperDownloader:
    """è®ºæ–‡ä¸‹è½½å™¨"""
    
    def __init__(self, output_dir: str = DEFAULT_OUTPUT_DIR):
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)
        self.arxiv_api = ArxivAPI()
        self.semantic_api = SemanticScholarAPI()
        self.downloaded = []
        self.failed = []
    
    def sanitize_filename(self, name: str) -> str:
        """æ¸…ç†æ–‡ä»¶åï¼Œç§»é™¤éæ³•å­—ç¬¦"""
        name = html.unescape(name)
        name = re.sub(r'[<>:"/\\|?*]', '', name)
        name = re.sub(r'\s+', '_', name)
        if len(name) > 150:
            name = name[:150]
        return name

    @staticmethod
    def _normalize_title(text: str) -> str:
        text = html.unescape(text or "")
        text = text.lower()
        text = re.sub(r'\s+', ' ', text)
        text = re.sub(r'[^a-z0-9 ]+', ' ', text)
        return ' '.join(text.split())

    @classmethod
    def _title_similarity(cls, a: str, b: str) -> float:
        return SequenceMatcher(None, cls._normalize_title(a), cls._normalize_title(b)).ratio()

    @classmethod
    def _token_coverage(cls, query: str, candidate: str) -> float:
        query_tokens = {t for t in cls._normalize_title(query).split() if len(t) > 2}
        if not query_tokens:
            return 0.0
        candidate_tokens = set(cls._normalize_title(candidate).split())
        return len(query_tokens & candidate_tokens) / len(query_tokens)

    def _find_existing_pdf(self, title: str, subfolder: str = "") -> Optional[Path]:
        target_dir = self.output_dir / subfolder if subfolder else self.output_dir
        if not target_dir.exists():
            return None

        best_path: Optional[Path] = None
        best_score = 0.0
        for pdf_path in target_dir.glob("*.pdf"):
            cand_title = pdf_path.stem.replace("_", " ")
            sim = self._title_similarity(title, cand_title)
            cov = self._token_coverage(title, cand_title)
            score = (sim + cov) / 2
            if score > best_score:
                best_score = score
                best_path = pdf_path

        if best_path and best_score >= 0.7:
            return best_path
        return None
    
    def search_paper(self, title: str) -> Optional[Paper]:
        """æœç´¢è®ºæ–‡ï¼Œä¼˜å…ˆä½¿ç”¨Semantic Scholarï¼Œå¤±è´¥åˆ™ç”¨arXiv"""
        print(f"\nğŸ” æœç´¢: {title[:60]}...")
        
        # å…ˆå°è¯• Semantic Scholarï¼ˆä¼˜å…ˆï¼‰
        papers = self.semantic_api.search(title, limit=5)

        # å¦‚æœæ²¡æ‰¾åˆ°æˆ–æ²¡æœ‰å¯ç”¨PDFï¼Œå†å°è¯• arXivï¼ˆä»…ä½œä¸ºå…œåº•ï¼‰
        if not papers or not any(p.pdf_url for p in papers):
            arxiv_papers = self.arxiv_api.search(title, max_results=5)
            papers.extend(arxiv_papers)
        
        if not papers:
            print(f"  âŒ æœªæ‰¾åˆ°åŒ¹é…çš„è®ºæ–‡")
            return None
        
        # åŸºäºâ€œæ ‡é¢˜ç›¸ä¼¼åº¦ + tokenè¦†ç›–ç‡â€é€‰æœ€ä½³å€™é€‰ï¼Œé¿å…ä¸‹è½½åˆ°æ— å…³è®ºæ–‡
        with_pdf = [p for p in papers if p.pdf_url]
        candidates = with_pdf if with_pdf else papers
        best = max(
            candidates,
            key=lambda p: (self._title_similarity(title, p.title) + self._token_coverage(title, p.title)) / 2,
        )
        best_sim = self._title_similarity(title, best.title)
        best_cov = self._token_coverage(title, best.title)
        best_score = (best_sim + best_cov) / 2

        if best_score < 0.72 or best_cov < 0.7:
            print(
                f"  âŒ å€™é€‰æ ‡é¢˜åŒ¹é…åº¦è¿‡ä½(sim={best_sim:.2f}, cov={best_cov:.2f})ï¼Œè·³è¿‡: {best.title}"
            )
            return None

        if not best.pdf_url:
            print(f"  âš ï¸  æ‰¾åˆ°è®ºæ–‡ä½†æ— å¼€æ”¾PDF: {best.title}")
            return best

        return best
    
    def download_pdf(self, paper: Paper, subfolder: str = "") -> bool:
        """ä¸‹è½½è®ºæ–‡PDF"""
        if not paper.pdf_url:
            print(f"  âš ï¸  æ— å¯ç”¨PDFé“¾æ¥")
            return False
        
        # å‡†å¤‡ä¿å­˜è·¯å¾„
        target_dir = self.output_dir / subfolder if subfolder else self.output_dir
        target_dir.mkdir(parents=True, exist_ok=True)
        
        filename = self.sanitize_filename(paper.title) + ".pdf"
        filepath = target_dir / filename
        
        if filepath.exists():
            print(f"  âœ… å·²å­˜åœ¨: {filename}")
            return True
        
        print(f"  â¬‡ï¸  ä¸‹è½½ä¸­: {paper.pdf_url}")
        
        try:
            req = urllib.request.Request(
                paper.pdf_url,
                headers={"User-Agent": "Mozilla/5.0 (Academic Paper Downloader)"}
            )
            with urllib.request.urlopen(req, timeout=60, context=SSL_CONTEXT) as response:
                content = response.read()
            
            # éªŒè¯æ˜¯å¦ä¸ºPDF
            if not content.startswith(b'%PDF'):
                print(f"  âŒ ä¸‹è½½å†…å®¹ä¸æ˜¯æœ‰æ•ˆçš„PDF")
                return False
            
            with open(filepath, 'wb') as f:
                f.write(content)
            
            size_mb = len(content) / (1024 * 1024)
            print(f"  âœ… ä¸‹è½½æˆåŠŸ: {filename} ({size_mb:.2f} MB)")
            return True
            
        except Exception as e:
            print(f"  âŒ ä¸‹è½½å¤±è´¥: {e}")
            return False
    
    def process_single(self, title: str, subfolder: str = "") -> bool:
        """å¤„ç†å•ç¯‡è®ºæ–‡"""
        existing = self._find_existing_pdf(title, subfolder=subfolder)
        if existing:
            print(f"\nâœ… å·²å­˜åœ¨(åŒ¹é…): {existing.name}")
            self.downloaded.append(title)
            return True

        paper = self.search_paper(title)
        if paper:
            success = self.download_pdf(paper, subfolder)
            if success:
                self.downloaded.append(title)
            else:
                self.failed.append((title, "ä¸‹è½½å¤±è´¥"))
            return success
        else:
            self.failed.append((title, "æœªæ‰¾åˆ°"))
            return False
    
    def process_batch(self, titles: list[str], delay: float = 1.0):
        """æ‰¹é‡å¤„ç†"""
        print(f"\nğŸ“š å¼€å§‹æ‰¹é‡ä¸‹è½½ {len(titles)} ç¯‡è®ºæ–‡...")
        
        for i, title in enumerate(titles, 1):
            print(f"\n[{i}/{len(titles)}]")
            self.process_single(title)
            time.sleep(delay)
        
        self.print_summary()
    
    def print_summary(self):
        """æ‰“å°ä¸‹è½½æ‘˜è¦"""
        print("\n" + "=" * 60)
        print("ğŸ“Š ä¸‹è½½æ‘˜è¦")
        print("=" * 60)
        print(f"âœ… æˆåŠŸä¸‹è½½: {len(self.downloaded)} ç¯‡")
        print(f"âŒ ä¸‹è½½å¤±è´¥: {len(self.failed)} ç¯‡")
        
        if self.failed:
            print("\nå¤±è´¥åˆ—è¡¨:")
            for title, reason in self.failed:
                print(f"  - {title[:50]}... ({reason})")


def parse_readme_for_papers(readme_path: str) -> list[tuple[str, str]]:
    """
    ä»readme.mdè§£æè®ºæ–‡æ ‡é¢˜å’Œå¯¹åº”çš„åˆ†ç±»
    è¿”å›: [(è®ºæ–‡æ ‡é¢˜, ç›®å½•å), ...]
    """
    with open(readme_path, 'r', encoding='utf-8') as f:
        content = f.read()
    
    def extract_title(markdown_line: str) -> Optional[str]:
        # å»æ‰å¼€å¤´çš„ "- "
        item = markdown_line[2:].strip()
        # å»æ‰å½¢å¦‚ [å¾…ä¸‹è½½] çš„æ ‡è®°
        item = re.sub(r'\s*\[[^\]]+\]\s*', ' ', item).strip()
        # ä»…ä¿ç•™æ ‡é¢˜éƒ¨åˆ†ï¼ˆå»æ‰åé¢çš„æè¿°ï¼‰
        item = item.split(' - ')[0].strip()
        # å»æ‰æœ«å°¾åŒ…å«å¹´ä»½çš„æ‹¬å·ï¼Œä¾‹å¦‚ "(2019)"ã€"(Meta, 2024)"ã€"(Akbarpour & Li, 2020)"
        item = re.sub(r'\s*\([^)]*\d{4}[^)]*\)\s*$', '', item).strip()
        if len(item) <= 5:
            return None
        return item

    papers: list[tuple[str, str]] = []
    current_section = ""
    
    lines = content.split('\n')
    for line in lines:
        # æ£€æµ‹ä¸€çº§æ ‡é¢˜ (## 1. ç«ä»·ç­–ç•¥)
        section_match = re.match(r'^## (\d+)\.\s+', line)
        if section_match:
            section_num = section_match.group(1)
            current_section = CATEGORY_MAP.get(section_num, "")
            continue
        
        # è§£æè®ºæ–‡è¡Œ: - è®ºæ–‡æ ‡é¢˜ (å¹´ä»½) - æè¿°
        # å…è®¸åŒ…å« [å¾…ä¸‹è½½] çš„æ¡ç›®ï¼ˆç”¨äºé‡è¯•ä¸‹è½½/è¡¥å…¨ï¼‰
        if line.startswith('- ') and current_section:
            title = extract_title(line)
            if title:
                papers.append((title, current_section))
    
    return papers


def interactive_mode(downloader: PaperDownloader):
    """äº¤äº’å¼æ¨¡å¼"""
    print("\nğŸ“– è®ºæ–‡ä¸‹è½½å·¥å…· - äº¤äº’æ¨¡å¼")
    print(f"è¾“å‡ºç›®å½•: {downloader.output_dir}")
    print("è¾“å…¥è®ºæ–‡æ ‡é¢˜è¿›è¡Œæœç´¢ä¸‹è½½ï¼Œè¾“å…¥ 'q' é€€å‡º\n")
    
    # æ˜¾ç¤ºå¯ç”¨åˆ†ç±»
    print("å¯ç”¨åˆ†ç±»:")
    for num, name in CATEGORY_MAP.items():
        print(f"  {num}: {name}")
    print()
    
    while True:
        title = input("ğŸ” è¯·è¾“å…¥è®ºæ–‡æ ‡é¢˜: ").strip()
        if title.lower() == 'q':
            break
        if not title:
            continue
        
        category = input("ğŸ“ é€‰æ‹©åˆ†ç±» (1-5, å›è½¦è·³è¿‡): ").strip()
        subfolder = CATEGORY_MAP.get(category, "")
        
        downloader.process_single(title, subfolder=subfolder)
    
    if downloader.downloaded:
        downloader.print_summary()


def main():
    import argparse
    
    parser = argparse.ArgumentParser(description="è®ºæ–‡æ‰¹é‡ä¸‹è½½å·¥å…·")
    parser.add_argument("--batch", type=str, help="åŒ…å«è®ºæ–‡æ ‡é¢˜çš„æ–‡ä»¶ï¼ˆæ¯è¡Œä¸€ä¸ªæ ‡é¢˜ï¼‰")
    parser.add_argument("--from-readme", action="store_true", help="ä»readme.mdè§£æè®ºæ–‡æ ‡é¢˜")
    parser.add_argument("--output", type=str, default=DEFAULT_OUTPUT_DIR, help="è¾“å‡ºç›®å½•")
    parser.add_argument("--delay", type=float, default=1.5, help="è¯·æ±‚é—´éš”ï¼ˆç§’ï¼‰")
    parser.add_argument("-y", "--yes", action="store_true", help="è‡ªåŠ¨ç¡®è®¤ä¸‹è½½ï¼Œè·³è¿‡ç¡®è®¤æç¤º")
    
    args = parser.parse_args()
    
    downloader = PaperDownloader(output_dir=args.output)
    
    if args.from_readme:
        readme_path = Path(__file__).parent / "readme.md"
        if not readme_path.exists():
            print(f"âŒ æœªæ‰¾åˆ° {readme_path}")
            return
        
        papers = parse_readme_for_papers(str(readme_path))
        print(f"\nğŸ“‹ ä» readme.md è§£æå‡º {len(papers)} ç¯‡è®ºæ–‡")
        
        for title, category in papers:
            print(f"  - [{category}] {title[:50]}...")
        
        confirm = 'y' if args.yes else input("\nç¡®è®¤ä¸‹è½½è¿™äº›è®ºæ–‡? (y/n): ").strip().lower()
        if confirm == 'y':
            for title, category in papers:
                downloader.process_single(title, subfolder=category)
                time.sleep(args.delay)
            downloader.print_summary()
    
    elif args.batch:
        with open(args.batch, 'r', encoding='utf-8') as f:
            titles = [line.strip() for line in f if line.strip()]
        downloader.process_batch(titles, delay=args.delay)
    
    else:
        interactive_mode(downloader)


if __name__ == "__main__":
    main()
