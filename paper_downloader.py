#!/usr/bin/env python3
"""
è®ºæ–‡æ‰¹é‡ä¸‹è½½å·¥å…·
æ”¯æŒä» arXiv å’Œ Semantic Scholar æœç´¢å¹¶ä¸‹è½½è®ºæ–‡PDF

ä½¿ç”¨æ–¹æ³•:
    python paper_downloader.py                    # äº¤äº’å¼å•ç¯‡æœç´¢ä¸‹è½½
    python paper_downloader.py --batch papers.txt # æ‰¹é‡ä¸‹è½½ï¼ˆä»æ–‡ä»¶è¯»å–æ ‡é¢˜åˆ—è¡¨ï¼‰
    python paper_downloader.py --from-readme      # ä»readme.mdè§£æè®ºæ–‡æ ‡é¢˜å¹¶ä¸‹è½½
"""

import os
import re
import json
import time
import urllib.request
import urllib.parse
import urllib.error
from pathlib import Path
from dataclasses import dataclass
from typing import Optional
import ssl

# åˆ›å»ºä¸éªŒè¯SSLçš„ä¸Šä¸‹æ–‡ï¼ˆæŸäº›ç¯å¢ƒå¯èƒ½éœ€è¦ï¼‰
SSL_CONTEXT = ssl.create_default_context()
SSL_CONTEXT.check_hostname = False
SSL_CONTEXT.verify_mode = ssl.CERT_NONE

# ç›®å½•æ˜ å°„ï¼šæ¿å—ç¼–å· -> ç›®å½•å
CATEGORY_MAP = {
    "1": "1_ç«ä»·ç­–ç•¥",
    "2": "2_æ‹å–æœºåˆ¶è®¾è®¡", 
    "3": "3_LLMä¸ç»æµä»£ç†",
    "4": "4_åšå¼ˆè®ºåŸºç¡€",
    "5": "5_åŸºå‡†ä¸ç»¼è¿°",
}

# é»˜è®¤è¾“å‡ºç›®å½•
DEFAULT_OUTPUT_DIR = "./Ad_Bidding_Auction_Mechanisms"


@dataclass
class Paper:
    """è®ºæ–‡ä¿¡æ¯ç»“æ„"""
    title: str
    authors: list[str]
    year: Optional[int]
    pdf_url: Optional[str]
    arxiv_id: Optional[str]
    source: str  # 'arxiv' or 'semantic_scholar'
    abstract: Optional[str] = None


class SemanticScholarAPI:
    """Semantic Scholar API å°è£…"""
    BASE_URL = "https://api.semanticscholar.org/graph/v1"
    
    def __init__(self, api_key: Optional[str] = None):
        self.api_key = api_key
        self.headers = {"Accept": "application/json"}
        if api_key:
            self.headers["x-api-key"] = api_key
    
    def search(self, query: str, limit: int = 5) -> list[Paper]:
        """æœç´¢è®ºæ–‡"""
        params = urllib.parse.urlencode({
            "query": query,
            "limit": limit,
            "fields": "title,authors,year,openAccessPdf,externalIds,abstract"
        })
        url = f"{self.BASE_URL}/paper/search?{params}"
        
        try:
            req = urllib.request.Request(url, headers=self.headers)
            with urllib.request.urlopen(req, timeout=30, context=SSL_CONTEXT) as response:
                data = json.loads(response.read().decode('utf-8'))
        except urllib.error.HTTPError as e:
            print(f"âš ï¸  Semantic Scholar API é”™è¯¯: {e.code}")
            return []
        except Exception as e:
            print(f"âš ï¸  è¯·æ±‚å¤±è´¥: {e}")
            return []
        
        papers = []
        for item in data.get("data", []):
            arxiv_id = None
            if item.get("externalIds"):
                arxiv_id = item["externalIds"].get("ArXiv")
            
            pdf_url = None
            if item.get("openAccessPdf"):
                pdf_url = item["openAccessPdf"].get("url")
            
            papers.append(Paper(
                title=item.get("title", ""),
                authors=[a.get("name", "") for a in item.get("authors", [])],
                year=item.get("year"),
                pdf_url=pdf_url,
                arxiv_id=arxiv_id,
                source="semantic_scholar",
                abstract=item.get("abstract")
            ))
        
        return papers


class ArxivAPI:
    """arXiv API å°è£…"""
    BASE_URL = "http://export.arxiv.org/api/query"
    
    def search(self, query: str, max_results: int = 5) -> list[Paper]:
        """æœç´¢è®ºæ–‡"""
        params = urllib.parse.urlencode({
            "search_query": f"all:{query}",
            "start": 0,
            "max_results": max_results,
            "sortBy": "relevance"
        })
        url = f"{self.BASE_URL}?{params}"
        
        try:
            req = urllib.request.Request(url)
            with urllib.request.urlopen(req, timeout=30) as response:
                content = response.read().decode('utf-8')
        except Exception as e:
            print(f"âš ï¸  arXiv API è¯·æ±‚å¤±è´¥: {e}")
            return []
        
        # ç®€å•çš„XMLè§£æï¼ˆé¿å…ä¾èµ–é¢å¤–åº“ï¼‰
        papers = []
        entries = re.findall(r'<entry>(.*?)</entry>', content, re.DOTALL)
        
        for entry in entries:
            # æå–æ ‡é¢˜
            title_match = re.search(r'<title>(.*?)</title>', entry, re.DOTALL)
            title = title_match.group(1).strip().replace('\n', ' ') if title_match else ""
            
            # æå–ä½œè€…
            authors = re.findall(r'<name>(.*?)</name>', entry)
            
            # æå–IDå’ŒPDFé“¾æ¥
            id_match = re.search(r'<id>http://arxiv.org/abs/(.*?)</id>', entry)
            arxiv_id = id_match.group(1) if id_match else None
            pdf_url = f"https://arxiv.org/pdf/{arxiv_id}.pdf" if arxiv_id else None
            
            # æå–å¹´ä»½
            published_match = re.search(r'<published>(\d{4})', entry)
            year = int(published_match.group(1)) if published_match else None
            
            # æå–æ‘˜è¦
            abstract_match = re.search(r'<summary>(.*?)</summary>', entry, re.DOTALL)
            abstract = abstract_match.group(1).strip() if abstract_match else None
            
            if title:
                papers.append(Paper(
                    title=title,
                    authors=authors,
                    year=year,
                    pdf_url=pdf_url,
                    arxiv_id=arxiv_id,
                    source="arxiv",
                    abstract=abstract
                ))
        
        return papers


class PaperDownloader:
    """è®ºæ–‡ä¸‹è½½å™¨"""
    
    def __init__(self, output_dir: str = DEFAULT_OUTPUT_DIR):
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)
        self.arxiv_api = ArxivAPI()
        self.semantic_api = SemanticScholarAPI()
        self.downloaded = []
        self.failed = []
    
    def sanitize_filename(self, name: str) -> str:
        """æ¸…ç†æ–‡ä»¶åï¼Œç§»é™¤éæ³•å­—ç¬¦"""
        name = re.sub(r'[<>:"/\\|?*]', '', name)
        name = re.sub(r'\s+', '_', name)
        if len(name) > 150:
            name = name[:150]
        return name
    
    def search_paper(self, title: str) -> Optional[Paper]:
        """æœç´¢è®ºæ–‡ï¼Œä¼˜å…ˆä½¿ç”¨Semantic Scholarï¼Œå¤±è´¥åˆ™ç”¨arXiv"""
        print(f"\nğŸ” æœç´¢: {title[:60]}...")
        
        # å…ˆå°è¯• Semantic Scholar
        papers = self.semantic_api.search(title, limit=3)
        
        # å¦‚æœæ²¡æ‰¾åˆ°æˆ–æ²¡æœ‰PDFï¼Œå°è¯• arXiv
        if not papers or not any(p.pdf_url for p in papers):
            arxiv_papers = self.arxiv_api.search(title, max_results=3)
            papers.extend(arxiv_papers)
        
        if not papers:
            print(f"  âŒ æœªæ‰¾åˆ°åŒ¹é…çš„è®ºæ–‡")
            return None
        
        # æ‰¾åˆ°æœ€ä½³åŒ¹é…ï¼ˆæœ‰PDFçš„ä¼˜å…ˆï¼‰
        best = None
        for p in papers:
            if p.pdf_url:
                best = p
                break
        
        if not best:
            best = papers[0]
            print(f"  âš ï¸  æ‰¾åˆ°è®ºæ–‡ä½†æ— å¼€æ”¾PDF: {best.title}")
            return best
        
        return best
    
    def download_pdf(self, paper: Paper, subfolder: str = "") -> bool:
        """ä¸‹è½½è®ºæ–‡PDF"""
        if not paper.pdf_url:
            print(f"  âš ï¸  æ— å¯ç”¨PDFé“¾æ¥")
            return False
        
        # å‡†å¤‡ä¿å­˜è·¯å¾„
        target_dir = self.output_dir / subfolder if subfolder else self.output_dir
        target_dir.mkdir(parents=True, exist_ok=True)
        
        filename = self.sanitize_filename(paper.title) + ".pdf"
        filepath = target_dir / filename
        
        if filepath.exists():
            print(f"  âœ… å·²å­˜åœ¨: {filename}")
            return True
        
        print(f"  â¬‡ï¸  ä¸‹è½½ä¸­: {paper.pdf_url}")
        
        try:
            req = urllib.request.Request(
                paper.pdf_url,
                headers={"User-Agent": "Mozilla/5.0 (Academic Paper Downloader)"}
            )
            with urllib.request.urlopen(req, timeout=60, context=SSL_CONTEXT) as response:
                content = response.read()
            
            # éªŒè¯æ˜¯å¦ä¸ºPDF
            if not content.startswith(b'%PDF'):
                print(f"  âŒ ä¸‹è½½å†…å®¹ä¸æ˜¯æœ‰æ•ˆçš„PDF")
                return False
            
            with open(filepath, 'wb') as f:
                f.write(content)
            
            size_mb = len(content) / (1024 * 1024)
            print(f"  âœ… ä¸‹è½½æˆåŠŸ: {filename} ({size_mb:.2f} MB)")
            return True
            
        except Exception as e:
            print(f"  âŒ ä¸‹è½½å¤±è´¥: {e}")
            return False
    
    def process_single(self, title: str, subfolder: str = "") -> bool:
        """å¤„ç†å•ç¯‡è®ºæ–‡"""
        paper = self.search_paper(title)
        if paper:
            success = self.download_pdf(paper, subfolder)
            if success:
                self.downloaded.append(title)
            else:
                self.failed.append((title, "ä¸‹è½½å¤±è´¥"))
            return success
        else:
            self.failed.append((title, "æœªæ‰¾åˆ°"))
            return False
    
    def process_batch(self, titles: list[str], delay: float = 1.0):
        """æ‰¹é‡å¤„ç†"""
        print(f"\nğŸ“š å¼€å§‹æ‰¹é‡ä¸‹è½½ {len(titles)} ç¯‡è®ºæ–‡...")
        
        for i, title in enumerate(titles, 1):
            print(f"\n[{i}/{len(titles)}]")
            self.process_single(title)
            time.sleep(delay)
        
        self.print_summary()
    
    def print_summary(self):
        """æ‰“å°ä¸‹è½½æ‘˜è¦"""
        print("\n" + "=" * 60)
        print("ğŸ“Š ä¸‹è½½æ‘˜è¦")
        print("=" * 60)
        print(f"âœ… æˆåŠŸä¸‹è½½: {len(self.downloaded)} ç¯‡")
        print(f"âŒ ä¸‹è½½å¤±è´¥: {len(self.failed)} ç¯‡")
        
        if self.failed:
            print("\nå¤±è´¥åˆ—è¡¨:")
            for title, reason in self.failed:
                print(f"  - {title[:50]}... ({reason})")


def parse_readme_for_papers(readme_path: str) -> list[tuple[str, str]]:
    """
    ä»readme.mdè§£æè®ºæ–‡æ ‡é¢˜å’Œå¯¹åº”çš„åˆ†ç±»
    è¿”å›: [(è®ºæ–‡æ ‡é¢˜, ç›®å½•å), ...]
    """
    with open(readme_path, 'r', encoding='utf-8') as f:
        content = f.read()
    
    papers = []
    current_section = ""
    
    lines = content.split('\n')
    for line in lines:
        # æ£€æµ‹ä¸€çº§æ ‡é¢˜ (## 1. ç«ä»·ç­–ç•¥)
        section_match = re.match(r'^## (\d+)\.\s+', line)
        if section_match:
            section_num = section_match.group(1)
            current_section = CATEGORY_MAP.get(section_num, "")
            continue
        
        # è§£æè®ºæ–‡è¡Œ: - è®ºæ–‡æ ‡é¢˜ (å¹´ä»½) - æè¿°
        # è·³è¿‡å·²æ ‡è®°ä¸º[å¾…ä¸‹è½½]çš„è®ºæ–‡ï¼ˆå®ƒä»¬å·²ç»åœ¨ç›®å½•ä¸­åŒ¹é…å¤±è´¥ï¼‰
        if line.startswith('- ') and '[å¾…ä¸‹è½½]' not in line:
            # æå–è®ºæ–‡æ ‡é¢˜ï¼ˆåœ¨ç¬¬ä¸€ä¸ªæ‹¬å·ä¹‹å‰çš„éƒ¨åˆ†ï¼‰
            paper_match = re.match(r'^- (.+?)\s*\(', line)
            if paper_match and current_section:
                title = paper_match.group(1).strip()
                if len(title) > 5:  # è¿‡æ»¤å¤ªçŸ­çš„
                    papers.append((title, current_section))
    
    return papers


def interactive_mode(downloader: PaperDownloader):
    """äº¤äº’å¼æ¨¡å¼"""
    print("\nğŸ“– è®ºæ–‡ä¸‹è½½å·¥å…· - äº¤äº’æ¨¡å¼")
    print(f"è¾“å‡ºç›®å½•: {downloader.output_dir}")
    print("è¾“å…¥è®ºæ–‡æ ‡é¢˜è¿›è¡Œæœç´¢ä¸‹è½½ï¼Œè¾“å…¥ 'q' é€€å‡º\n")
    
    # æ˜¾ç¤ºå¯ç”¨åˆ†ç±»
    print("å¯ç”¨åˆ†ç±»:")
    for num, name in CATEGORY_MAP.items():
        print(f"  {num}: {name}")
    print()
    
    while True:
        title = input("ğŸ” è¯·è¾“å…¥è®ºæ–‡æ ‡é¢˜: ").strip()
        if title.lower() == 'q':
            break
        if not title:
            continue
        
        category = input("ğŸ“ é€‰æ‹©åˆ†ç±» (1-5, å›è½¦è·³è¿‡): ").strip()
        subfolder = CATEGORY_MAP.get(category, "")
        
        downloader.process_single(title, subfolder=subfolder)
    
    if downloader.downloaded:
        downloader.print_summary()


def main():
    import argparse
    
    parser = argparse.ArgumentParser(description="è®ºæ–‡æ‰¹é‡ä¸‹è½½å·¥å…·")
    parser.add_argument("--batch", type=str, help="åŒ…å«è®ºæ–‡æ ‡é¢˜çš„æ–‡ä»¶ï¼ˆæ¯è¡Œä¸€ä¸ªæ ‡é¢˜ï¼‰")
    parser.add_argument("--from-readme", action="store_true", help="ä»readme.mdè§£æè®ºæ–‡æ ‡é¢˜")
    parser.add_argument("--output", type=str, default=DEFAULT_OUTPUT_DIR, help="è¾“å‡ºç›®å½•")
    parser.add_argument("--delay", type=float, default=1.5, help="è¯·æ±‚é—´éš”ï¼ˆç§’ï¼‰")
    parser.add_argument("-y", "--yes", action="store_true", help="è‡ªåŠ¨ç¡®è®¤ä¸‹è½½ï¼Œè·³è¿‡ç¡®è®¤æç¤º")
    
    args = parser.parse_args()
    
    downloader = PaperDownloader(output_dir=args.output)
    
    if args.from_readme:
        readme_path = Path(__file__).parent / "readme.md"
        if not readme_path.exists():
            print(f"âŒ æœªæ‰¾åˆ° {readme_path}")
            return
        
        papers = parse_readme_for_papers(str(readme_path))
        print(f"\nğŸ“‹ ä» readme.md è§£æå‡º {len(papers)} ç¯‡è®ºæ–‡")
        
        for title, category in papers:
            print(f"  - [{category}] {title[:50]}...")
        
        confirm = 'y' if args.yes else input("\nç¡®è®¤ä¸‹è½½è¿™äº›è®ºæ–‡? (y/n): ").strip().lower()
        if confirm == 'y':
            for title, category in papers:
                downloader.process_single(title, subfolder=category)
                time.sleep(args.delay)
            downloader.print_summary()
    
    elif args.batch:
        with open(args.batch, 'r', encoding='utf-8') as f:
            titles = [line.strip() for line in f if line.strip()]
        downloader.process_batch(titles, delay=args.delay)
    
    else:
        interactive_mode(downloader)


if __name__ == "__main__":
    main()
